{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data comes from IBM Business Analytics and it is comprised of 4 Databases with different amount of data from each customer. The goal of this notebook is to compile all the data and clean it so it is ready for the modeling steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataframes\n",
    "df1=pd.read_excel(\"Telco_customer_churn.xlsx\")\n",
    "df2=pd.read_excel(\"Telco_customer_churn_status.xlsx\")\n",
    "df3=pd.read_excel(\"Telco_customer_churn_services.xlsx\")\n",
    "df4=pd.read_excel(\"Telco_customer_churn_demographics.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mergin dataframes into one\n",
    "df1_2=pd.merge(df1, df2, left_on='CustomerID', right_on='Customer ID', how='left').drop('Customer ID', axis=1)\n",
    "df3_4=pd.merge(df3, df4, left_on='Customer ID', right_on='Customer ID', how='left')\n",
    "customer_df=pd.merge(df1_2, df3_4, left_on='CustomerID', right_on='Customer ID', how='left').drop('Customer ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates from DF and renaming the columns \n",
    "customer_df=customer_df.T.drop_duplicates().T\n",
    "customer_df.columns=[column.lower().strip(\"_x\").strip(\"_y\").replace(\" \",\"_\")for column in customer_df.columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dups(lists):\n",
    "    \"\"\"\n",
    "    Takes in a list of duplicated columns with the same name and removes the duplicated column and creates a\n",
    "    new column with a new name and values assigned.\n",
    "    Once that is complete it drops column from dataframe.\n",
    "    \"\"\"\n",
    "    for i in range(len(lists)):\n",
    "        customer_df[\"tel_\"+lists[i]]=customer_df[lists[i]].iloc[:,1:2]\n",
    "        customer_df.drop(lists[i],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dup=[\"internet_service\",\"online_securit\",\"online_backup\",\"multiple_lines\",\"streaming_tv\",\"payment_method\",\"total_charges\",\"contract\",\"streaming_movies\",\"churn_reason\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates that were not caught by the previous function.\n",
    "remove_dups(remove_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Redundant columns\n",
    "customer_df.drop([\"lat_long\",\"customerid\",\"churn_label\",\"count\",\"countr\",\"state\",\"quarter\",],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3d09f234fd45>:8: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  customer_df.tel_total_chargs=hold\n"
     ]
    }
   ],
   "source": [
    "#Cleaning missing values from \"total_charge\" column\n",
    "hold=[]\n",
    "for x in customer_df.index:\n",
    "    if type(customer_df.tel_total_charges[x])== str:\n",
    "        hold.append(0)\n",
    "    else:\n",
    "        hold.append(customer_df.tel_total_charges[x])\n",
    "customer_df.tel_total_chargs=hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning age \n",
    "customer_df.age=pd.cut(x=customer_df['age'], bins=[10,20,30,40, 50,60,70,80], labels=[\"10\",\"20\",\"30\",\"40\", \"50\",\"60\",\"70\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling NA values for churn reason\n",
    "customer_df.tel_churn_reason=customer_df.tel_churn_reason.fillna(\"No reason given\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the column of Churn Category, by comparing if the customer churned or not. If the customer didnt churn, Fill it with \"Not Churn\" to add the category that the customer did not leave, therefore filling the missing valuees. If the customer left, then fill it with \"No reason\" since we dont have a reason of why they left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions=[\n",
    "    (customer_df[\"churn_categor\"].isna())& (customer_df[\"churn_value\"]==0),            \n",
    "    (customer_df[\"churn_categor\"].isna())& (customer_df[\"churn_value\"]==1),\n",
    "]\n",
    "choises=[\n",
    "    \"Not Churned\",\n",
    "    \"No Reason\",\n",
    "\n",
    "]\n",
    "customer_df.churn_categor=np.select(conditions,choises,default=customer_df.churn_categor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def to_binary(column):\n",
    "    \"\"\"\n",
    "    Takes in a column and turns its value from a Yes or No to a binary. \n",
    "    \"\"\"\n",
    "    conditions=[\n",
    "        (customer_df[column].str.lower()=='yes'),            \n",
    "        (customer_df[column].str.lower()=='no'),\n",
    "    ]\n",
    "    choises=[\n",
    "        1,\n",
    "        0,\n",
    "\n",
    "    ]\n",
    "    customer_df[column]=np.select(conditions,choises,default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reason_cleanup(column):\n",
    "    \"\"\"\n",
    "    Takes in the tel_churn_reason column and creates its own main categories as to why a customer churned. \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    reasons={\n",
    "        \"Competition Offers\":[\"Competitor had better devices\",\"Competitor made better offer\",\"Competitor offered more data\",\"Competitor offered higher download speeds\"],\n",
    "        \"Customer Satisfaction\":[\"Attitude of support person\",\"Attitude of service provider\",\"Poor expertise of online support\",\"Poor expertise of phone support\"],\n",
    "        \"Pricing\":[\"Price too high\",\"Lack of affordable download/upload speed\"],\n",
    "        \"Charges and Fees\":[\"Long distance charges\",\"Extra data charges\"],\n",
    "        \"Product and Services\":[\"Product dissatisfaction\",\"Network reliability\",\"Service dissatisfaction\",\"Limited range of services\",\"Lack of self-service on Website\"],\n",
    "        \"External Factors\":[\"Moved\",\"Deceased\"],\n",
    "        \"Unkown\":[\"Don't know\"],\n",
    "        \"Did Not Churn\":[\"No reason given\"]\n",
    "    }\n",
    "    reason=[]\n",
    "    for i in customer_df[column].index:\n",
    "        for key in reasons.keys():\n",
    "            if customer_df[column][i]in reasons[key]:\n",
    "                reason.append(key)\n",
    "    return reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re categorizing the churn reason for better interpretation.\n",
    "customer_df[\"tel_churn_reason\"]=reason_cleanup(\"tel_churn_reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting columns into binary\n",
    "col_to_bin=[\"senior_citizen\",\"partner\",\"dependents\",\"phone_service\",\"device_protection\",\"tech_support\",\"paperless_billing\",\"referred_a_friend\",\"device_protection_plan\",\"premium_tech_support\",\"streaming_music\",\"unlimited_data\",\"under_30\",\"tel_internet_service\",\"tel_online_securit\",\"tel_online_backup\",\"tel_multiple_lines\",\"tel_streaming_tv\",\"tel_streaming_movies\"]\n",
    "for x in col_to_bin:\n",
    "    to_binary(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The clean Dataframe is saved to be used for Visualizations in the Visualizations notebook.\n",
    "customer_df.to_csv(\"clean_customer_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Preparing Dataframe for modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part the categorical variables are made into dummies making sure to drop the first value. In the cleaning process, some features are not in the right format for modeling. I change the type of the DF to Float so all the data is able to be processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.drop([\"tel_churn_reason\",\"churn_categor\",\"customer_status\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df=pd.get_dummies(customer_df,columns=[\"cit\",\"zip_code\",\"latitude\",\"longitude\",\"gender\",\"offer\",\"internet_type\",\"age\",\"tel_payment_method\",\"tel_contract\"],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df=customer_df.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.to_csv(\"modeling_customer_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Vanilla model, I want to explore a linear regression model. Since a Linear regression is the most basic and simple model to run without any parameter tuning, it can help me get an idea of where my data is at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 57561 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "client = Client(processes=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=customer_df[\"churn_value\"]\n",
    "X=customer_df[customer_df.columns[customer_df.columns!=\"churn_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping a random state to keep concistency on each model\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=2020, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_test=scaler.fit_transform(X_test)\n",
    "X_train=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 306 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 823 epochs took 252 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=-1, random_state=40, solver='sag',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "lr=LogisticRegression(solver=\"sag\" ,max_iter=1000, random_state=40, verbose=1, n_jobs=-1)\n",
    "\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9973375931842385\n",
      "Test Accuracy: 0.9198012775017743\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy:\",lr.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\",lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9198012775017743\n",
      "Logistic Regression F1: 0.855683269476373\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression Accuracy: {}'.format(metrics.accuracy_score(y_test, y_pred_lr)))\n",
    "print('Logistic Regression F1: {}'.format(metrics.f1_score(y_test,y_pred_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has performed very well for a vanilla model. This model will be explored further in the Modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
